p2p-Cortical: A model of cortical electronic prosthetic technologies
====================================================================
This code implements computational model or ‘virtual patient’, based on the neurophysiological architecture of visual cortex, which successfully predicts the perceptual experience of participants across a wide range of previously published human cortical stimulation studies describing the location, size, brightness and spatiotemporal shape of electrically induced percepts in humans. 

Simulations such as the above are likely to be critical for providing realistic estimates of prosthetic vision, thus providing regulatory bodies with guidance into what sort of visual tests are appropriate for evaluating prosthetic performance, and improving current and future technology.

If you use p2p-cortical in a scholarly publication, please cite as:

Fine, I., Boynton, G.M., A virtual patient simulation modeling the neural and perceptual effects of human visual cortical stimulation, from pulse trains to percepts. Scientific Reports, 2024 14, 17400.
[https://www.nature.com/articles/s41598-024-65337-1#citeas](https://doi.org/10.1038/s41598-024-65337-1)

Most of the code in this library was originally written by Ione Fine and Geoffrey Boynten and reviewed, edited, added to and cleaned up by Eirini Schoinas.

Getting started
===============

The bulk of the routines in p2p-cortical are in the Methods file p2p_c.
Example files to get you started are described in the site wiki.




